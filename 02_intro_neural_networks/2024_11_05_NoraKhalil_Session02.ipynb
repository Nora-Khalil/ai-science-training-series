{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b883bd9b-f7ab-409d-b512-495c0b6502e6",
   "metadata": {},
   "source": [
    "# Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "224c2ffa-691a-4e04-a9bb-8e63290f8fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "import numpy \n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d847191-8d2d-4ee8-a195-be4c6f7002fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the data\n",
    "\n",
    "training_data = torchvision.datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2fa8b2a-d002-42c5-bcb1-0f2b88b68846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions\n",
    "\n",
    "def train_one_epoch(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # forward pass\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y) # X is input (images), y is label (0-9)\n",
    "        \n",
    "        # backward pass calculates gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # take one step with these gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # resets the gradients \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "def evaluate(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - some NN pieces behave differently during training\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    loss, correct = 0, 0\n",
    "\n",
    "    # We can save computation and memory by not calculating gradients here - we aren't optimizing \n",
    "    with torch.no_grad():\n",
    "        # loop over all of the batches\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            loss += loss_fn(pred, y).item()\n",
    "            # how many are correct in this batch? Tracking for accuracy \n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    loss /= num_batches\n",
    "    correct /= size\n",
    "    \n",
    "    accuracy = 100*correct\n",
    "    return accuracy, loss\n",
    "\n",
    "def show_failures(model, dataloader, maxtoshow=10):\n",
    "    model.eval()\n",
    "    batch = next(iter(dataloader))\n",
    "    predictions = model(batch[0])\n",
    "    \n",
    "    rounded = predictions.argmax(1) #dimensions=1\n",
    "    errors = rounded!=batch[1] #X, y so y = label\n",
    "    print('Showing max', maxtoshow, 'first failures. '\n",
    "          'The predicted class is shown first and the correct class in parentheses.')\n",
    "    ii = 0\n",
    "    plt.figure(figsize=(maxtoshow, 1))\n",
    "    for i in range(batch[0].shape[0]):\n",
    "        if ii>=maxtoshow:\n",
    "            break\n",
    "        if errors[i]:\n",
    "            plt.subplot(1, maxtoshow, ii+1)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(batch[0][i,0,:,:], cmap=\"gray\")\n",
    "            plt.title(\"%d (%d)\" % (rounded[i], batch[1][i]))\n",
    "            ii = ii + 1\n",
    "\n",
    "class NonlinearClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layers_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "           # nn.Dropout(0.2),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "           # nn.Dropout(0.2),\n",
    "            nn.Linear(50, 10)\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.layers_stack(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class LinearClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # First, we need to convert the input image to a vector by using \n",
    "        # nn.Flatten(). For MNIST, it means the second dimension 28*28 becomes 784.\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Here, we add a fully connected (\"dense\") layer that has 28 x 28 = 784 input nodes \n",
    "        #(one for each pixel in the input image) and 10 output nodes (for probabilities of each class).\n",
    "        self.layer_1 = nn.Linear(28*28, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.layer_1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02124b37-2ba8-401c-991c-cfa8ad9a8a93",
   "metadata": {},
   "source": [
    "## How does *batch size* effect the quality of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27870307-bd7b-4f8b-aed4-4a1ea358a929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** Batch size = 32 ********************\n",
      "Epoch 0: training loss: 0.3008524134270847, accuracy: 90.67708333333333\n",
      "Epoch 0: val. loss: 0.3002712804178397, val. accuracy: 90.61666666666667\n",
      "Epoch 1: training loss: 0.16908632882746558, accuracy: 94.91875\n",
      "Epoch 1: val. loss: 0.17410429557661217, val. accuracy: 94.69999999999999\n",
      "Epoch 2: training loss: 0.13618672937620432, accuracy: 95.93958333333333\n",
      "Epoch 2: val. loss: 0.14839355806509655, val. accuracy: 95.5\n",
      "Epoch 3: training loss: 0.11744997482653707, accuracy: 96.4125\n",
      "Epoch 3: val. loss: 0.13087551322206856, val. accuracy: 95.89166666666667\n",
      "Epoch 4: training loss: 0.0946422436911768, accuracy: 97.1875\n",
      "Epoch 4: val. loss: 0.11517379763908685, val. accuracy: 96.575\n",
      "Test loss: 0.1177, test accuracy: 96.43%\n",
      "*************** Batch size = 64 ********************\n",
      "Epoch 0: training loss: 0.4371504230300585, accuracy: 87.20052083333333\n",
      "Epoch 0: val. loss: 0.4506137156486511, val. accuracy: 86.92708333333333\n",
      "Epoch 1: training loss: 0.2833182856192191, accuracy: 91.7734375\n",
      "Epoch 1: val. loss: 0.29980543235937757, val. accuracy: 91.19791666666667\n",
      "Epoch 2: training loss: 0.21495536724726358, accuracy: 93.7421875\n",
      "Epoch 2: val. loss: 0.23497534451385338, val. accuracy: 92.85416666666667\n",
      "Epoch 3: training loss: 0.18061560848106942, accuracy: 94.74479166666666\n",
      "Epoch 3: val. loss: 0.20040780951579412, val. accuracy: 94.03125\n",
      "Epoch 4: training loss: 0.15059107894077897, accuracy: 95.63802083333334\n",
      "Epoch 4: val. loss: 0.17512542724609376, val. accuracy: 94.82291666666667\n",
      "Test loss: 0.1580, test accuracy: 95.07%\n",
      "*************** Batch size = 128 ********************\n",
      "Epoch 0: training loss: 1.4738738760352135, accuracy: 57.887369791666664\n",
      "Epoch 0: val. loss: 1.4719648520151773, val. accuracy: 58.2421875\n",
      "Epoch 1: training loss: 0.6017766387512287, accuracy: 82.78645833333333\n",
      "Epoch 1: val. loss: 0.5991803805033366, val. accuracy: 82.82552083333333\n",
      "Epoch 2: training loss: 0.4016013201326132, accuracy: 88.818359375\n",
      "Epoch 2: val. loss: 0.40007423038283985, val. accuracy: 88.37239583333333\n",
      "Epoch 3: training loss: 0.3313939508671562, accuracy: 90.55338541666667\n",
      "Epoch 3: val. loss: 0.3302421326438586, val. accuracy: 90.078125\n",
      "Epoch 4: training loss: 0.29127595157672964, accuracy: 91.66015625\n",
      "Epoch 4: val. loss: 0.2926082871854305, val. accuracy: 91.2109375\n",
      "Test loss: 0.2844, test accuracy: 91.29%\n",
      "*************** Batch size = 256 ********************\n",
      "Epoch 0: training loss: 2.2800504316886268, accuracy: 22.749837239583336\n",
      "Epoch 0: val. loss: 2.2792544762293496, val. accuracy: 22.216796875\n",
      "Epoch 1: training loss: 2.159225876132647, accuracy: 40.81217447916667\n",
      "Epoch 1: val. loss: 2.155940184990565, val. accuracy: 41.796875\n",
      "Epoch 2: training loss: 1.6415350226064522, accuracy: 52.392578125\n",
      "Epoch 2: val. loss: 1.6340402215719223, val. accuracy: 53.3203125\n",
      "Epoch 3: training loss: 0.8886699924866358, accuracy: 72.74983723958334\n",
      "Epoch 3: val. loss: 0.8801834434270859, val. accuracy: 72.96549479166666\n",
      "Epoch 4: training loss: 0.5903386448820432, accuracy: 82.36083984375\n",
      "Epoch 4: val. loss: 0.5859731162587801, val. accuracy: 82.45442708333334\n",
      "Test loss: 0.5691, test accuracy: 82.84%\n",
      "*************** Batch size = 512 ********************\n",
      "Epoch 0: training loss: 2.2977173572931533, accuracy: 11.291963377416073\n",
      "Epoch 0: val. loss: 2.2994794130325316, val. accuracy: 11.187957689178193\n",
      "Epoch 1: training loss: 2.285476048787435, accuracy: 23.173957273652086\n",
      "Epoch 1: val. loss: 2.2867223739624025, val. accuracy: 22.335231895850285\n",
      "Epoch 2: training loss: 2.2661331983713002, accuracy: 40.01525940996948\n",
      "Epoch 2: val. loss: 2.2668996095657348, val. accuracy: 40.45972335231896\n",
      "Epoch 3: training loss: 2.2290848035078783, accuracy: 51.256358087487286\n",
      "Epoch 3: val. loss: 2.229202914237976, val. accuracy: 51.383238405207486\n",
      "Epoch 4: training loss: 2.140223582585653, accuracy: 54.04374364191251\n",
      "Epoch 4: val. loss: 2.139411759376526, val. accuracy: 54.536208299430434\n",
      "Test loss: 2.1360, test accuracy: 54.27%\n",
      "CPU times: user 57min 22s, sys: 2.14 s, total: 57min 24s\n",
      "Wall time: 2min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_sizes = [32, 64, 128, 256, 512]\n",
    "    # batch_size = 128\n",
    "for batch_size in batch_sizes: \n",
    "\n",
    "    train_size = int(0.8 * len(training_data))  # 80% for training\n",
    "    val_size = len(training_data) - train_size  # Remaining 20% for validation\n",
    "    training_data, validation_data = torch.utils.data.random_split(training_data, [train_size, val_size], generator=torch.Generator().manual_seed(55))\n",
    "    \n",
    "\n",
    "    print(f'*************** Batch size = {batch_size} ********************') \n",
    "    nonlinear_model = NonlinearClassifier()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(nonlinear_model.parameters(), lr=0.05)\n",
    "    \n",
    "    # The dataloader makes our dataset iterable \n",
    "    train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size)\n",
    "    val_dataloader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size)\n",
    "    \n",
    "    \n",
    "    \n",
    "    epochs = 5\n",
    "    train_acc_all = []\n",
    "    val_acc_all = []\n",
    "    for j in range(epochs):\n",
    "        train_one_epoch(train_dataloader, nonlinear_model, loss_fn, optimizer)\n",
    "        \n",
    "        # checking on the training loss and accuracy once per epoch\n",
    "        acc, loss = evaluate(train_dataloader, nonlinear_model, loss_fn)\n",
    "        train_acc_all.append(acc)\n",
    "        print(f\"Epoch {j}: training loss: {loss}, accuracy: {acc}\")\n",
    "        \n",
    "        # checking on the validation loss and accuracy once per epoch\n",
    "        val_acc, val_loss = evaluate(val_dataloader, nonlinear_model, loss_fn)\n",
    "        val_acc_all.append(val_acc)\n",
    "        print(f\"Epoch {j}: val. loss: {val_loss}, val. accuracy: {val_acc}\")\n",
    "\n",
    "    #finally, evaluate how it performs against the test data: \n",
    "    batch_size_test = 256\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size_test)\n",
    "    acc_test, loss_test = evaluate(test_dataloader, nonlinear_model, loss_fn)\n",
    "    print(\"Test loss: %.4f, test accuracy: %.2f%%\" % (loss_test, acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7046a892-5847-42aa-9016-c4ab15b68d1e",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25896c39-3a6d-483b-aa4d-1a716400bc96",
   "metadata": {},
   "source": [
    "It looks like the smaller the batch size, the higher the final test accuracy. As batch size increased, test accuracy decreased. However, the smaller the batch size, the more computationally intensive it is to train the model. So opting for a small-to-medium batch size (i.e. 64, 128) can be a good \"sweet\" spot for acheiving high accuracy but also maintaining manageable wall-time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bb2555-30ea-42e2-92c0-8ec9f977c725",
   "metadata": {},
   "source": [
    "## How does *learning rates* effect the quality of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f2d3310-9334-4711-a613-14e7a81ad3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** Learning Rate = 0.01 ********************\n",
      "Epoch 0: training loss: 2.2963698045994207, accuracy: 13.19938962360122\n",
      "Epoch 0: val. loss: 2.2967900691493863, val. accuracy: 12.487283825025433\n",
      "Epoch 1: training loss: 2.285515523538357, accuracy: 15.895218718209565\n",
      "Epoch 1: val. loss: 2.2859163822666293, val. accuracy: 15.106815869786368\n",
      "Epoch 2: training loss: 2.2712665146928495, accuracy: 25.406917599186162\n",
      "Epoch 2: val. loss: 2.2716645348456597, val. accuracy: 24.974567650050865\n",
      "Epoch 3: training loss: 2.2512357913381686, accuracy: 23.963631739572737\n",
      "Epoch 3: val. loss: 2.251570301671182, val. accuracy: 23.957273652085455\n",
      "Epoch 4: training loss: 2.219492443208772, accuracy: 28.560528992878943\n",
      "Epoch 4: val. loss: 2.2196949297381985, val. accuracy: 28.713123092573756\n",
      "Test loss: 2.2184, test accuracy: 28.77%\n",
      "*************** Learning Rate = 0.05 ********************\n",
      "Epoch 0: training loss: 2.276081251375603, accuracy: 10.610395803528851\n",
      "Epoch 0: val. loss: 2.2755908870697024, val. accuracy: 11.315956770502225\n",
      "Epoch 1: training loss: 2.060442960623539, accuracy: 36.97345414083612\n",
      "Epoch 1: val. loss: 2.061847038269043, val. accuracy: 39.09726636999365\n",
      "Epoch 2: training loss: 1.3941039497202092, accuracy: 55.1581624542998\n",
      "Epoch 2: val. loss: 1.4019487714767456, val. accuracy: 57.27908455181182\n",
      "Epoch 3: training loss: 0.8836378744154265, accuracy: 71.63408043236369\n",
      "Epoch 3: val. loss: 0.8811249661445618, val. accuracy: 72.75905912269549\n",
      "Epoch 4: training loss: 0.6387340552879103, accuracy: 79.72500397393101\n",
      "Epoch 4: val. loss: 0.6387561392784119, val. accuracy: 80.29243483788939\n",
      "Test loss: 0.6228, test accuracy: 80.61%\n",
      "*************** Learning Rate = 0.1 ********************\n",
      "Epoch 0: training loss: 2.1698777856706064, accuracy: 41.231992051664186\n",
      "Epoch 0: val. loss: 2.16901638507843, val. accuracy: 42.391736193881606\n",
      "Epoch 1: training loss: 1.115169588523575, accuracy: 56.14505712866369\n",
      "Epoch 1: val. loss: 1.0993986755609513, val. accuracy: 58.00556217719507\n",
      "Epoch 2: training loss: 0.6421765428555163, accuracy: 78.39046199701937\n",
      "Epoch 2: val. loss: 0.6382961019873619, val. accuracy: 78.78426698450536\n",
      "Epoch 3: training loss: 0.4965382686144189, accuracy: 83.70591157476403\n",
      "Epoch 3: val. loss: 0.49693949669599535, val. accuracy: 84.14779499404052\n",
      "Epoch 4: training loss: 0.4081044649776024, accuracy: 87.09388971684054\n",
      "Epoch 4: val. loss: 0.41657057255506513, val. accuracy: 86.8891537544696\n",
      "Test loss: 0.4061, test accuracy: 87.39%\n",
      "*************** Learning Rate = 0.2 ********************\n",
      "Epoch 0: training loss: 1.392961409356859, accuracy: 50.161450571286636\n",
      "Epoch 0: val. loss: 1.4230908155441284, val. accuracy: 49.32935916542474\n",
      "Epoch 1: training loss: 0.7926210325861734, accuracy: 73.63387978142076\n",
      "Epoch 1: val. loss: 0.8337337411940098, val. accuracy: 73.37307501241928\n",
      "Epoch 2: training loss: 0.4781428315336742, accuracy: 85.17138599105813\n",
      "Epoch 2: val. loss: 0.5207760725170374, val. accuracy: 84.05365126676601\n",
      "Epoch 3: training loss: 0.33242405265096636, accuracy: 89.96522603079981\n",
      "Epoch 3: val. loss: 0.3927355706691742, val. accuracy: 88.37555886736214\n",
      "Epoch 4: training loss: 0.28446141429363736, accuracy: 91.38102334823647\n",
      "Epoch 4: val. loss: 0.35857938043773174, val. accuracy: 89.56780923994039\n",
      "Test loss: 0.3196, test accuracy: 90.55%\n",
      "CPU times: user 14min 58s, sys: 391 ms, total: 14min 58s\n",
      "Wall time: 39.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.2]\n",
    "    # batch_size = 128\n",
    "for lr_ in learning_rates: \n",
    "    \n",
    "    train_size = int(0.8 * len(training_data))  # 80% for training\n",
    "    val_size = len(training_data) - train_size  # Remaining 20% for validation\n",
    "    # print(f'train_size: {train_size}')\n",
    "    # print(f'val_size: {val_size}')\n",
    "    training_data, validation_data = torch.utils.data.random_split(training_data, [train_size, val_size], generator=torch.Generator().manual_seed(55))\n",
    "    \n",
    "    batch_size = 128 #keep constant\n",
    "    print(f'*************** Learning Rate = {lr_} ********************') \n",
    "    nonlinear_model = NonlinearClassifier()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(nonlinear_model.parameters(), lr=lr_)\n",
    "    \n",
    "    # The dataloader makes our dataset iterable \n",
    "    train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size)\n",
    "    val_dataloader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size)\n",
    "    \n",
    "    \n",
    "    \n",
    "    epochs = 5\n",
    "    train_acc_all = []\n",
    "    val_acc_all = []\n",
    "    for j in range(epochs):\n",
    "        train_one_epoch(train_dataloader, nonlinear_model, loss_fn, optimizer)\n",
    "        \n",
    "        # checking on the training loss and accuracy once per epoch\n",
    "        acc, loss = evaluate(train_dataloader, nonlinear_model, loss_fn)\n",
    "        train_acc_all.append(acc)\n",
    "        print(f\"Epoch {j}: training loss: {loss}, accuracy: {acc}\")\n",
    "        \n",
    "        # checking on the validation loss and accuracy once per epoch\n",
    "        val_acc, val_loss = evaluate(val_dataloader, nonlinear_model, loss_fn)\n",
    "        val_acc_all.append(val_acc)\n",
    "        print(f\"Epoch {j}: val. loss: {val_loss}, val. accuracy: {val_acc}\")\n",
    "\n",
    "    #finally, evaluate how it performs against the test data: \n",
    "    batch_size_test = 256\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size_test)\n",
    "    acc_test, loss_test = evaluate(test_dataloader, nonlinear_model, loss_fn)\n",
    "    print(\"Test loss: %.4f, test accuracy: %.2f%%\" % (loss_test, acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffc3ea3-5257-4b8d-a43f-2d591f2e867a",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cabc62-a4cf-40dd-a4aa-77cdb84c9c6b",
   "metadata": {},
   "source": [
    "In this case, a larger learning rate increased test accuracy by the final epoch. Not sure if there is an error in my code, since each time my model starts its first epoch, it has an accuracy that's better than the first epoch of the last learning rate test. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58a5d06-f042-4c24-8739-e08cfe0b0854",
   "metadata": {},
   "source": [
    "## How does *activation function* effect the quality of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b927104b-8f32-46f0-bf2d-c820095f0737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** Nonlinear Model ********************\n",
      "Epoch 0: training loss: 2.2846088876911237, accuracy: 10.852352119236143\n",
      "Epoch 0: val. loss: 2.288375597733718, val. accuracy: 10.800744878957168\n",
      "Epoch 1: training loss: 2.2394960300595153, accuracy: 31.936034777208512\n",
      "Epoch 1: val. loss: 2.2445620206686168, val. accuracy: 30.54003724394786\n",
      "Epoch 2: training loss: 2.085206672257068, accuracy: 34.29591678310822\n",
      "Epoch 2: val. loss: 2.0966731218191295, val. accuracy: 33.14711359404097\n",
      "Epoch 3: training loss: 1.713658886797288, accuracy: 46.685297314081666\n",
      "Epoch 3: val. loss: 1.7454931735992432, val. accuracy: 43.637492240844196\n",
      "Epoch 4: training loss: 1.209678187089808, accuracy: 65.11411271541687\n",
      "Epoch 4: val. loss: 1.263515564111563, val. accuracy: 62.321539416511484\n",
      "Test loss: 1.2112, test accuracy: 64.08%\n",
      "*************** Linear Model ********************\n",
      "Epoch 0: training loss: 2.308105433859476, accuracy: 9.452639751552795\n",
      "Epoch 0: val. loss: 2.301988861777566, val. accuracy: 11.016291698991466\n",
      "Epoch 1: training loss: 2.308105433859476, accuracy: 9.452639751552795\n",
      "Epoch 1: val. loss: 2.301988861777566, val. accuracy: 11.016291698991466\n",
      "Epoch 2: training loss: 2.308105433859476, accuracy: 9.452639751552795\n",
      "Epoch 2: val. loss: 2.301988861777566, val. accuracy: 11.016291698991466\n",
      "Epoch 3: training loss: 2.308105433859476, accuracy: 9.452639751552795\n",
      "Epoch 3: val. loss: 2.301988861777566, val. accuracy: 11.016291698991466\n",
      "Epoch 4: training loss: 2.308105433859476, accuracy: 9.452639751552795\n",
      "Epoch 4: val. loss: 2.301988861777566, val. accuracy: 11.016291698991466\n",
      "Test loss: 2.3075, test accuracy: 9.25%\n",
      "CPU times: user 3min 48s, sys: 127 ms, total: 3min 48s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "########### Nonlinear Model\n",
    "\n",
    "train_size = int(0.8 * len(training_data))  # 80% for training\n",
    "val_size = len(training_data) - train_size  # Remaining 20% for validation\n",
    "# print(f'train_size: {train_size}')\n",
    "# print(f'val_size: {val_size}')\n",
    "training_data, validation_data = torch.utils.data.random_split(training_data, [train_size, val_size], generator=torch.Generator().manual_seed(55))\n",
    "\n",
    "batch_size = 128 #keep constant\n",
    "print(f'*************** Nonlinear Model ********************') \n",
    "nonlinear_model = NonlinearClassifier()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(nonlinear_model.parameters(), lr=0.05)\n",
    "\n",
    "# The dataloader makes our dataset iterable \n",
    "train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size)\n",
    "val_dataloader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "train_acc_all = []\n",
    "val_acc_all = []\n",
    "for j in range(epochs):\n",
    "    train_one_epoch(train_dataloader, nonlinear_model, loss_fn, optimizer)\n",
    "    \n",
    "    # checking on the training loss and accuracy once per epoch\n",
    "    acc, loss = evaluate(train_dataloader, nonlinear_model, loss_fn)\n",
    "    train_acc_all.append(acc)\n",
    "    print(f\"Epoch {j}: training loss: {loss}, accuracy: {acc}\")\n",
    "    \n",
    "    # checking on the validation loss and accuracy once per epoch\n",
    "    val_acc, val_loss = evaluate(val_dataloader, nonlinear_model, loss_fn)\n",
    "    val_acc_all.append(val_acc)\n",
    "    print(f\"Epoch {j}: val. loss: {val_loss}, val. accuracy: {val_acc}\")\n",
    "\n",
    "#finally, evaluate how it performs against the test data: \n",
    "batch_size_test = 256\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size_test)\n",
    "acc_test, loss_test = evaluate(test_dataloader, nonlinear_model, loss_fn)\n",
    "print(\"Test loss: %.4f, test accuracy: %.2f%%\" % (loss_test, acc_test))\n",
    "\n",
    "############ Linear Model\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(training_data))  # 80% for training\n",
    "val_size = len(training_data) - train_size  # Remaining 20% for validation\n",
    "# print(f'train_size: {train_size}')\n",
    "# print(f'val_size: {val_size}')\n",
    "training_data, validation_data = torch.utils.data.random_split(training_data, [train_size, val_size], generator=torch.Generator().manual_seed(55))\n",
    "\n",
    "batch_size = 128 #keep constant\n",
    "print(f'*************** Linear Model ********************') \n",
    "linear_model = LinearClassifier()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(nonlinear_model.parameters(), lr=0.05)\n",
    "\n",
    "# The dataloader makes our dataset iterable \n",
    "train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size)\n",
    "val_dataloader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size)\n",
    "\n",
    "epochs = 5\n",
    "train_acc_all = []\n",
    "val_acc_all = []\n",
    "for j in range(epochs):\n",
    "    train_one_epoch(train_dataloader, linear_model, loss_fn, optimizer)\n",
    "    \n",
    "    # checking on the training loss and accuracy once per epoch\n",
    "    acc, loss = evaluate(train_dataloader, linear_model, loss_fn)\n",
    "    train_acc_all.append(acc)\n",
    "    print(f\"Epoch {j}: training loss: {loss}, accuracy: {acc}\")\n",
    "    \n",
    "    # checking on the validation loss and accuracy once per epoch\n",
    "    val_acc, val_loss = evaluate(val_dataloader, linear_model, loss_fn)\n",
    "    val_acc_all.append(val_acc)\n",
    "    print(f\"Epoch {j}: val. loss: {val_loss}, val. accuracy: {val_acc}\")\n",
    "    \n",
    "#finally, evaluate how it performs against the test data: \n",
    "batch_size_test = 256\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size_test)\n",
    "acc_test, loss_test = evaluate(test_dataloader, linear_model, loss_fn)\n",
    "print(\"Test loss: %.4f, test accuracy: %.2f%%\" % (loss_test, acc_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e720898-a26b-429e-88bc-49f858f3dfed",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a634376f-0bf6-4bb3-8669-a92746f6824e",
   "metadata": {},
   "source": [
    "A purely linear activation function produces a model with low accuracy. Creating a neural network that has layers of both linear and nonlinear activation functions increases accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8a54bb-4fbc-493e-82e3-a47ba3686e56",
   "metadata": {},
   "source": [
    "# Bonus: What is a learning rate scheduler?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e9d53-c708-4981-af56-4caf6e320799",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f130876-fe8c-41f1-9f0e-361a29be91f8",
   "metadata": {},
   "source": [
    "A learning rate scheduler is a scheduled framework that changes the learning rate between epochs or iterations during training. It is often used to describe a framework that gradually decays the learning rate during training. \n",
    "\n",
    "resource: https://neptune.ai/blog/how-to-choose-a-learning-rate-scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3386f306-a247-4c59-97d1-955bce010782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda/2024-08-08",
   "language": "python",
   "name": "2024-08-08"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
